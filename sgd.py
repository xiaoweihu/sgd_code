import numpy as npimport mathfrom pylab import *import matplotlib.pyplot as pltdimension = 2init_theta = np.zeros(dimension)cov = np.identity(dimension)numRuns = 10numRounds = 1000000numMethods = 3#rounds_to_print = list(set([int(math.pow(10, .01*i)) for i in range(0, 100*int(math.log10(numRounds))+1)]))rounds_to_print = [1]for i in range(100*int(math.log10(numRounds))+1):    r = int(math.pow(10, .01*i))    if(r != rounds_to_print[-1]):        rounds_to_print.append(r)        test_theta = np.array([[[0. for i in range(len(rounds_to_print))] for j in range(numMethods)] for k in range(numRuns)])def unit_vector(vec):    mag = math.sqrt(sum(x*x for x in vec))    return np.array([x/mag for x in vec])    def generate_rand_vector(dimension):    vec = np.random.normal(0., 1., dimension)    return unit_vector(vec)                def loss(x, y, theta):    return 0.5 * pow( np.dot(x, theta) - y , 2)    def regret(theta, optimal_theta):    global cov    diff = theta - optimal_theta    return 0.5* np.dot(np.dot(diff,cov) , diff)# x: dimension*1 vector; y: scalardef SGD(x, y, gamma, lbd, theta):    theta = np.dot((1.-gamma*lbd) * np.identity(dimension) - gamma * cov, theta) + gamma * y * x + lbd * gamma * init_theta    return theta    def AccSGD(x, y, gamma, lbd, delta, theta, last_theta):    theta = np.dot((1.-gamma*lbd) * np.identity(dimension) - gamma * cov, theta+delta*(theta-last_theta)) + gamma * y * x + lbd * gamma * init_theta    return theta    def run_additive(rBias, rVariance):    # construct covariance matrix \Sigma, Gram Schmidt Algorithm    ortho_basis = [generate_rand_vector(dimension)]    for i in range(1, dimension):        vec = generate_rand_vector(dimension)        for j in range(i):            vec = vec - np.dot(vec, ortho_basis[j]) * ortho_basis[j]        vec = unit_vector(vec)        ortho_basis.append(vec)        cov = np.zeros((dimension, dimension))    for i in range(dimension):        v = ortho_basis[i].reshape((dimension, 1))        cov += v * v.T / float(math.pow(i+1, 3))    mean = np.zeros(dimension)        # Control BIAS TERM through optimal theta, inial theta    optimal_theta = generate_rand_vector(dimension)    init_theta = optimal_theta + rBias * generate_rand_vector(dimension)            # initialize gamma, lambda    trace_cov = 0.    for i in range(dimension):        trace_cov += cov[i][i]    gamma = 1./trace_cov/4.    lbd = 0.        for run in range(numRuns):        print "run: ", run                x = np.random.multivariate_normal(mean, cov, numRounds)        # Control VARIANCE TERM        noise = rVariance * np.random.normal(0., 1., numRounds)        y = np.dot(x, optimal_theta) + noise                for iMethod in range(numMethods):            theta = init_theta            last_theta = theta            theta_bar  = theta            idx_to_print = 0                        if iMethod == 0:  #avSGD                for n in range(numRounds):                    #theta = SGD(x[n], y[n], gamma, lbd, theta)                    theta = np.dot((1.-gamma*lbd) * np.identity(dimension) - gamma * cov, theta) + gamma * y[n] * x[n] + lbd * gamma * init_theta                                        theta_bar = float(n)/float(n+1.) * theta_bar + 1./float(n+1.) * theta                    if n+1 == rounds_to_print[idx_to_print]:                        test_theta[run][iMethod][idx_to_print] = regret(theta_bar, optimal_theta)                        idx_to_print += 1                                    elif iMethod == 1:  #accSGD                for n in range(numRounds):                    temp = theta                    #theta = AccSGD(x[n], y[n], gamma, lbd, 1. - 3./float(n+1.), theta, last_theta)                    delta = 1. - 3./float(n+1)                    theta = np.dot(((1.-gamma*lbd) * np.identity(dimension) - gamma * cov), theta+delta*(theta-last_theta)) + gamma * y[n] * x[n] + lbd * gamma * init_theta                    #theta = np.dot(((1.-gamma*lbd) * np.identity(dimension) - gamma * cov), theta+delta*(theta-last_theta)) + gamma * np.dot(cov, optimal_theta)+gamma*noise[n]*x[n] + lbd * gamma * init_theta                    last_theta = temp                                        if n+1 == rounds_to_print[idx_to_print]:                        test_theta[run][iMethod][idx_to_print] = regret(theta, optimal_theta)                        idx_to_print += 1                                    else:  #avaccSGD                for n in range(numRounds):                    temp = theta                    #theta = AccSGD(x[n], y[n], gamma, lbd, 1., theta, last_theta)                    delta = 1.                     theta = np.dot((1.-gamma*lbd) * np.identity(dimension) - gamma * cov, theta+delta*(theta-last_theta)) + gamma * y[n] * x[n] + lbd * gamma * init_theta                    #theta = np.dot(((1.-gamma*lbd) * np.identity(dimension) - gamma * cov), theta+delta*(theta-last_theta)) + gamma * np.dot(cov, optimal_theta)+gamma*noise[n]*x[n] + lbd * gamma * init_theta                    last_theta = temp                                            theta_bar = float(n)/float(n+1.) * theta_bar + 1./float(n+1.) * theta                    if n+1 == rounds_to_print[idx_to_print]:                        test_theta[run][iMethod][idx_to_print] = regret(theta_bar, optimal_theta)                        idx_to_print += 1                    avgregret = np.mean(test_theta, axis = 0)    fig, ax = plt.subplots()        ax.plot(log10(rounds_to_print), log10(avgregret[0]), label = "avSGD")    ax.plot(log10(rounds_to_print), log10(avgregret[1]), label = "accSGD")    ax.plot(log10(rounds_to_print), log10(avgregret[2]), label = "avaccSGD")    ax.legend(loc=0)    ax.set_xlabel('round number T (log)')    ax.set_ylabel('Average Regret (log)')    show()        return        run_additive(10., 10.)    